def dueling_ddqn(state_size: Tuple[int, int], action_size: int, learning_rate: float) -> Model:
    """ Duelling DDQN Neural Net """

    state_input = Input(shape=state_size)

    x = Dense(state_size[1], kernel_initializer="normal", activation="relu")(state_input)
    x = BatchNormalization()(x)

    x = Dense(state_size[1] // 2, kernel_initializer="normal", activation="relu")(x)
    x = BatchNormalization()(x)

    x = Dense(state_size[1], kernel_initializer="normal", activation="relu")(x)
    x = BatchNormalization()(x)

    x = Flatten()(x)

    # state value tower - V
    state_value = Dense(state_size[1], kernel_initializer="random_uniform", activation="relu")(x)

    state_value = Dense(1, kernel_initializer="random_uniform", activation="linear")(state_value)

    state_value = Lambda(lambda s: K.expand_dims(s[:, 0]), output_shape=(action_size,))(state_value)

    # action advantage tower - A
    action_advantage = Dense(state_size[1], kernel_initializer="normal", activation="linear")(x)

    action_advantage = Dense(action_size)(action_advantage)

    action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(action_size,))(
        action_advantage
    )

    # merge to state-action value function Q
    state_action_value = add([state_value, action_advantage])

    model = Model(input=state_input, output=state_action_value)
    model.compile(loss=huber_loss, optimizer=Adam(lr=learning_rate))

    return model
