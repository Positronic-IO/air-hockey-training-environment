{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, random\n",
    "path, _ = os.path.split(os.getcwd())\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" C51 DDQN Algorithm\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from typing import Tuple\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, Flatten, Input\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from rl.Agent import Agent\n",
    "from rl.helpers import huber_loss\n",
    "from utils import Observation, State, get_model_path\n",
    "\n",
    "\n",
    "class c51(Agent):\n",
    "\n",
    "    \"\"\" Reference: https://github.com/flyyufelix/C51-DDQN-Keras \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = (7,2)\n",
    "        self.action_size = 4\n",
    "\n",
    "        # these is hyper parameters for the DQN\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epsilon = 1.0\n",
    "        self.initial_epsilon = 1.0\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.batch_size = 32\n",
    "        self.observe = 2000\n",
    "        self.explore = 50000\n",
    "        self.frame_per_action = 4\n",
    "        self.update_target_freq = 3000 \n",
    "        self.timestep_per_train = 10000 # Number of timesteps between training interval\n",
    "\n",
    "        # Initialize Atoms\n",
    "        self.num_atoms = 51 # 51 for C51\n",
    "        self.v_max = 10 # Max possible score for Defend the center is 26 - 0.1*26 = 23.4\n",
    "        self.v_min = -10 # -0.1*26 - 1 = -3.6\n",
    "        self.delta_z = (self.v_max - self.v_min) / float(self.num_atoms - 1)\n",
    "        self.z = [self.v_min + i * self.delta_z for i in range(self.num_atoms)]\n",
    "        \n",
    "        # Create replay memory using deque\n",
    "        self.memory = deque()\n",
    "        self.max_memory = 50000 # number of previous transitions to remember\n",
    "\n",
    "        # Models for value distribution\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        print(self.model.summary())\n",
    "        self.batch_counter = 0\n",
    "        self.sync_counter = 0\n",
    "\n",
    "        self.version = \"0.1.0\"\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\" Create our DNN model for Q-value approximation \"\"\"\n",
    "\n",
    "        \n",
    "        state_input = Input(shape=(self.state_size)) \n",
    "        x = Dense(12, kernel_initializer=\"normal\", activation=\"relu\")(state_input)\n",
    "        x = Dense(30, kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
    "        x = Dense(20, kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        distribution_list = []\n",
    "        for i in range(self.action_size):\n",
    "            distribution_list.append(Dense(51, activation='softmax')(x))\n",
    "\n",
    "        model = Model(input=state_input, output=distribution_list)\n",
    "\n",
    "        model.compile(loss=huber_loss, optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def update_target_model(self) -> None:\n",
    "        \"\"\" Copy weights from model to target_model \"\"\"\n",
    "\n",
    "        print(\"Sync target model\")\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state: State) -> str:\n",
    "        \"\"\" Apply an espilon-greedy policy to pick next action \"\"\"\n",
    "\n",
    "        # Helps over fitting, encourages to exploration\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(self.env.actions)\n",
    "\n",
    "        # Compute rewards for any posible action\n",
    "        z = self.model.predict(np.array([state]), batch_size=1)\n",
    "        z_concat = np.vstack(z)\n",
    "        q = np.sum(np.multiply(z_concat, np.array(self.z)), axis=1)\n",
    "        # Pick action with the biggest Q value\n",
    "        idx = np.argmax(q[0])\n",
    "        return self.env.actions[idx]\n",
    "    \n",
    "    def remember(self, data: Observation) -> None:\n",
    "        \"\"\" Push data into memory for replay later \"\"\"\n",
    "\n",
    "        # Push data into observation and remove one from buffer\n",
    "        self.memory.append(data)\n",
    "\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory.popleft()\n",
    "\n",
    "    def update(self, data: Observation) -> None:\n",
    "        \"\"\" Experience replay \"\"\"\n",
    "\n",
    "        # Push data into observation and remove one from buffer\n",
    "        self.remember(data)\n",
    "\n",
    "        assert len(self.memory) < self.max_memory + 1, \"Max memory exceeded\"\n",
    "        \n",
    "        self.sync_counter += 1\n",
    "        if self.sync_counter > self.update_target_freq:\n",
    "            # Sync Target Model\n",
    "            self.update_target_model()\n",
    "            self.sync_counter = 0\n",
    "            \n",
    "        # Update model in intervals\n",
    "        self.batch_counter += 1\n",
    "        if self.batch_counter > self.timestep_per_train:\n",
    "                \n",
    "            # Reset Batch counter\n",
    "            self.batch_counter = 0\n",
    "\n",
    "            num_samples = min(self.batch_size * self.timestep_per_train, len(self.memory))\n",
    "            replay_samples = random.sample(self.memory, num_samples)\n",
    "\n",
    "            state_inputs = np.zeros(((num_samples,) + self.state_size)) \n",
    "            next_states = np.zeros(((num_samples,) + self.state_size)) \n",
    "            m_prob = [np.zeros((num_samples, self.num_atoms)) for i in range(self.action_size)]\n",
    "            action, reward, done = [], [], []\n",
    "            for i in range(num_samples):\n",
    "                state_inputs[i,:,:] = replay_samples[i][0]\n",
    "                action.append(replay_samples[i][1])\n",
    "                reward.append(replay_samples[i][2])\n",
    "                done.append(replay_samples[i][3])\n",
    "                next_states[i,:,:] = replay_samples[i][4]\n",
    "\n",
    "        \n",
    "            z = self.model.predict(next_states) # Return a list [32x51, 32x51, 32x51]\n",
    "            z_ = self.target_model.predict(next_states) # Return a list [32x51, 32x51, 32x51]\n",
    "\n",
    "            # Get Optimal Actions for the next states (from distribution z)\n",
    "            optimal_action_idxs = []\n",
    "            z_concat = np.vstack(z)\n",
    "            q = np.sum(np.multiply(z_concat, np.array(self.z)), axis=1) # length (num_atoms x num_actions)\n",
    "            q = q.reshape((num_samples, self.action_size), order='F')\n",
    "            optimal_action_idxs = np.argmax(q, axis=1)\n",
    "            \n",
    "\n",
    "            # Project Next State Value Distribution (of optimal action) to Current State\n",
    "            for i in range(num_samples):\n",
    "                if done[i]: # Terminal State\n",
    "                    # Distribution collapses to a single point\n",
    "                    Tz = min(self.v_max, max(self.v_min, reward[i]))\n",
    "                    bj = (Tz - self.v_min) / self.delta_z \n",
    "                    m_l, m_u = math.floor(bj), math.ceil(bj)\n",
    "                    m_prob[action.index(action[i])][i][int(m_l)] += (m_u - bj)\n",
    "                    m_prob[action.index(action[i])][i][int(m_u)] += (bj - m_l)\n",
    "                else:\n",
    "                    for j in range(self.num_atoms):\n",
    "                        Tz = min(self.v_max, max(self.v_min, reward[i] + self.gamma * self.z[j]))\n",
    "                        bj = (Tz - self.v_min) / self.delta_z \n",
    "                        m_l, m_u = math.floor(bj), math.ceil(bj)\n",
    "                        print(m_prob)\n",
    "                        m_prob[action.index(action[i])][i][int(m_l)] += z_[optimal_action_idxs[i]][i][j] * (m_u - bj)\n",
    "                        m_prob[action.index(action[i])][i][int(m_u)] += z_[optimal_action_idxs[i]][i][j] * (bj - m_l)\n",
    "\n",
    "            loss = self.model.fit(state_inputs, m_prob, batch_size=self.batch_size, nb_epoch=1, verbose=0)\n",
    "\n",
    "            return loss.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import State, Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import AirHockey\n",
    "env = AirHockey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/.local/share/virtualenvs/air-hockey-training-environment-zvB5Uyz9/lib/python3.6/site-packages/ipykernel_launcher.py:81: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_48 (InputLayer)           (None, 7, 2)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_477 (Dense)               (None, 7, 12)        36          input_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_478 (Dense)               (None, 7, 30)        390         dense_477[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_479 (Dense)               (None, 7, 20)        620         dense_478[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_73 (Flatten)            (None, 140)          0           dense_479[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_480 (Dense)               (None, 51)           7191        flatten_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_481 (Dense)               (None, 51)           7191        flatten_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_482 (Dense)               (None, 51)           7191        flatten_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_483 (Dense)               (None, 51)           7191        flatten_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 29,810\n",
      "Trainable params: 29,810\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lossagent = c51(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer 2, Agent 1\n",
      "Sync target model\n",
      "Sync target model\n",
      "Computer 2, Agent 2\n",
      "Computer 2, Agent 3\n",
      "Sync target model\n"
     ]
    }
   ],
   "source": [
    "loss = list()\n",
    "# Game loop\n",
    "init=True\n",
    "for i in range(10000):\n",
    "    # For first move, move in a random direction\n",
    "    if init:\n",
    "        action = str(np.random.choice(env.actions))\n",
    "\n",
    "        # Update game state\n",
    "        agent.move(action)\n",
    "\n",
    "        init = False\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # Now, let the model do all the work\n",
    "\n",
    "        # Current state\n",
    "        state = State(\n",
    "            agent_location=agent.location(),\n",
    "            puck_location=env.puck.location(),\n",
    "            puck_prev_location=env.puck.prev_location(),\n",
    "            puck_velocity=env.puck.velocity(),\n",
    "            opponent_location=env.opponent.location(),\n",
    "            opponent_prev_location=env.opponent.prev_location(),\n",
    "            opponent_velocity=env.opponent.velocity(),\n",
    "        )\n",
    "\n",
    "        # Determine next action\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        # Update game state\n",
    "        agent.move(action)\n",
    "\n",
    "        # New state\n",
    "        new_state = State(\n",
    "            agent_location=agent.location(),\n",
    "            puck_location=env.puck.location(),\n",
    "            puck_prev_location=env.puck.prev_location(),\n",
    "            puck_velocity=env.puck.velocity(),\n",
    "            opponent_location=env.opponent.location(),\n",
    "            opponent_prev_location=env.opponent.prev_location(),\n",
    "            opponent_velocity=env.opponent.velocity(),\n",
    "        )\n",
    "\n",
    "        # Record reward\n",
    "        reward = env.reward\n",
    "        done = env.done\n",
    "        # Observation of the game at the moment\n",
    "        observation = Observation(\n",
    "            state=state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            new_state=new_state,\n",
    "        )\n",
    "\n",
    "        # Update model\n",
    "        loss.append(agent.update(observation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
