{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "# Observation:\n",
    "#    state\n",
    "#    action\n",
    "#    reward\n",
    "#    done\n",
    "class Environment():\n",
    "\n",
    "    # Encoding:\n",
    "    # \"*\": agent position\n",
    "    # \" \": empty square\n",
    "    # \"T\": Table\n",
    "    # \"G\": Goal\n",
    "    def __init__(self):\n",
    "        self.agent_position = (0, 0)\n",
    "        self.map = [\n",
    "            [\" \", \" \", \" \", \" \", \"G\"],\n",
    "            [\" \", \" \", \" \", \" \", \" \"],\n",
    "            [\" \", \" \", \" \", \" \", \" \"],\n",
    "            [\" \", \" \", \" \", \" \", \" \"],\n",
    "            [\" \", \" \", \" \", \" \", \" \"]\n",
    "        ]\n",
    "\n",
    "    def draw_env(self):\n",
    "\n",
    "        x = self.agent_position[1]\n",
    "        y = self.agent_position[0]\n",
    "\n",
    "        last_token = self.map[y][x]\n",
    "\n",
    "        self.map[y][x] = \"*\"\n",
    "        print('----------------------')\n",
    "        for l in self.map:\n",
    "            print(l)\n",
    "\n",
    "        self.map[y][x] = last_token\n",
    "\n",
    "    # get the token from the current position\n",
    "    def get_token(self):\n",
    "        x = self.agent_position[1]\n",
    "        y = self.agent_position[0]\n",
    "\n",
    "        return self.map[y][x]\n",
    "\n",
    "    # reward mapping:\n",
    "    #  \" \" ->  0\n",
    "    #  \"T\" -> -1\n",
    "    #  \"G\" -> +1\n",
    "    def reward(self):\n",
    "        token = self.get_token()\n",
    "\n",
    "        if token == \" \":\n",
    "            return -0.1\n",
    "\n",
    "        if token == \"T\":\n",
    "            return -1\n",
    "\n",
    "        if token == \"G\":\n",
    "            return 100\n",
    "\n",
    "        return 0\n",
    "\n",
    "    # clamp a value between 0 and 4\n",
    "    def clamp_to_map(self, value):\n",
    "        if value < 0:\n",
    "            return 0\n",
    "\n",
    "        if value > 4:\n",
    "            return 4\n",
    "\n",
    "        return value\n",
    "\n",
    "    # action:\n",
    "    #   UP, DOWN, LEFT, RIGHT\n",
    "    #   state_position, action, reward, done\n",
    "    def next(self, action):\n",
    "\n",
    "        start_position = self.agent_position\n",
    "\n",
    "        x = self.agent_position[1]\n",
    "        y = self.agent_position[0]\n",
    "\n",
    "        # move the agent\n",
    "        if action == \"U\":\n",
    "            y = y - 1\n",
    "\n",
    "        if action == \"D\":\n",
    "            y = y + 1\n",
    "\n",
    "        if action == \"L\":\n",
    "            x = x - 1\n",
    "\n",
    "        if action == \"R\":\n",
    "            x = x + 1\n",
    "\n",
    "        # clamp it to the environment\n",
    "        x = self.clamp_to_map(x)\n",
    "        y = self.clamp_to_map(y)\n",
    "\n",
    "        self.agent_position = (y, x)\n",
    "\n",
    "        # determine the reward\n",
    "        reward = self.reward()\n",
    "\n",
    "        # is episode complete ?\n",
    "        token = self.get_token()\n",
    "        done = (token == \"1\" or token == \"T\")\n",
    "\n",
    "        return (start_position, action, reward, done)\n",
    "\n",
    "    # sets the agent position back to (0,0)\n",
    "    def reset(self):\n",
    "        self.agent_position = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent:\n",
    "#   model as Model\n",
    "#   state as State\n",
    "#   exploration as Float\n",
    "class Agent():\n",
    "\n",
    "    # needs a model to represent the rewards\n",
    "    def __init__(self, model, start_state, exploration):\n",
    "        self.model = model\n",
    "        self.state = start_state\n",
    "        self.exploration = exploration\n",
    "\n",
    "    # encoding\n",
    "    #   0 <- UP\n",
    "    #   1 <- RIGHT\n",
    "    #   2 <- LEFT\n",
    "    #   3 <- DOWN\n",
    "    def get_action(self, action_id):\n",
    "        if action_id == 0:\n",
    "            return \"U\"\n",
    "\n",
    "        if action_id == 1:\n",
    "            return \"R\"\n",
    "\n",
    "        if action_id == 2:\n",
    "            return \"D\"\n",
    "\n",
    "        return \"L\"\n",
    "\n",
    "    def next_action(self, env):\n",
    "        # test against the current exploration constant\n",
    "        prob = np.random.random()\n",
    "        action_id = None\n",
    "\n",
    "        if prob < self.exploration:\n",
    "            action_id = np.random.choice(4)\n",
    "        else:\n",
    "            action_id = self.model.predict(self.state)\n",
    "\n",
    "        # get the action token\n",
    "        action = self.get_action(action_id)\n",
    "        observation = env.next(action)\n",
    "\n",
    "        self.state = observation[0]\n",
    "\n",
    "        # return the observation\n",
    "        return observation\n",
    "\n",
    "    def reduce_exploration(self):\n",
    "        self.exploration = self.exploration ** 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Model():\n",
    "\n",
    "    def __init__(self, discount_factor, alpha):\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions_options = (\"U\", \"R\", \"D\", \"L\")\n",
    "        self.alpha = alpha\n",
    "        self.Q = {}\n",
    "\n",
    "        # initialize the actions for all states to zero\n",
    "        for y in range(5):\n",
    "            for x in range(5):\n",
    "                state = (y, x)\n",
    "\n",
    "                self.Q[state] = {}\n",
    "\n",
    "                for a in self.actions_options:\n",
    "                    self.Q[state][a] = 0\n",
    "\n",
    "\n",
    "    def predict(self, state):\n",
    "\n",
    "        actions = self.Q[state]\n",
    "\n",
    "        max_key = None\n",
    "        max_val = float('-inf')\n",
    "        for k, v in actions.items():\n",
    "            if v > max_val:\n",
    "                max_val = v\n",
    "                max_key = k\n",
    "\n",
    "        return max_key\n",
    "\n",
    "    def update(self, state, action, reward, state2, action2):\n",
    "        lastQ = self.Q[state][action]\n",
    "        self.Q[state][action] = self.Q[state][action] + self.alpha * (reward + self.discount_factor * self.Q[state2][action2] - self.Q[state][action])\n",
    "\n",
    "        return np.abs(lastQ - self.Q[state][action])\n",
    "\n",
    "    def policy(self, map):\n",
    "        policy = []\n",
    "\n",
    "        for y in range(5):\n",
    "            l = []\n",
    "\n",
    "            for x in range(5):\n",
    "                action = self.predict((y, x))\n",
    "\n",
    "                if map[y][x] != \" \":\n",
    "                    action = map[y][x]\n",
    "\n",
    "                l.append(action)\n",
    "\n",
    "            policy.append(l)\n",
    "\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(agent, env):\n",
    "\n",
    "    done = False\n",
    "\n",
    "    state = (0, 0)\n",
    "    observation = agent.next_action(env)\n",
    "    action = observation[1]\n",
    "\n",
    "    highest_delta = 0\n",
    "\n",
    "    while not done:\n",
    "        #  state_position, action, reward, done\n",
    "        observation = agent.next_action(env)\n",
    "\n",
    "        state2 = observation[0]\n",
    "        action2 = observation[1]\n",
    "        reward = observation[2]\n",
    "        done = observation[3]\n",
    "\n",
    "        delta = agent.model.update(state, action, reward, state2, action2)\n",
    "        highest_delta = max(delta, highest_delta)\n",
    "\n",
    "        state = state2\n",
    "        action = action2\n",
    "\n",
    "        if done:\n",
    "            agent.model.Q[state][action] = reward\n",
    "\n",
    "    return highest_delta\n",
    "\n",
    "def train_agent(agent, env):\n",
    "    done = False\n",
    "    max_iterations = 1000\n",
    "    i = 0\n",
    "\n",
    "    while not done:\n",
    "        change = episode(agent, env)\n",
    "        env.reset()\n",
    "        done = (change < 0.005)\n",
    "\n",
    "        i = i + 1\n",
    "        if i == max_iterations:\n",
    "            done = True\n",
    "\n",
    "        agent.reduce_exploration()\n",
    "\n",
    "    policy = agent.model.policy(env.map)\n",
    "\n",
    "    for l in policy:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_world = Environment()\n",
    "# agent_model = Model(discount_factor=0.98, alpha=0.1)\n",
    "# agent = Agent(agent_model, (0,0), 1.0)\n",
    "# train_agent(agent, grid_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'U': 61.05048114314936,\n",
       "  'R': 64.75281822208728,\n",
       "  'D': 58.69488618959197,\n",
       "  'L': 61.90057745957728},\n",
       " (0, 1): {'U': 66.4048918619842,\n",
       "  'R': 90.54349006476626,\n",
       "  'D': 63.114264483608935,\n",
       "  'L': 61.589424566442474},\n",
       " (0, 2): {'U': 86.78094798620326,\n",
       "  'R': 145.9536423765898,\n",
       "  'D': 80.55690534528776,\n",
       "  'L': 69.10096140282273},\n",
       " (0, 3): {'U': 141.35741470293084,\n",
       "  'R': 100,\n",
       "  'D': 101.51841338708829,\n",
       "  'L': 83.86474510752114},\n",
       " (0, 4): {'U': 0, 'R': 0, 'D': 0, 'L': 0},\n",
       " (1, 0): {'U': 61.26512971757272,\n",
       "  'R': 67.30804336610764,\n",
       "  'D': 53.04006561380301,\n",
       "  'L': 61.0224626456537},\n",
       " (1, 1): {'U': 70.92454467023319,\n",
       "  'R': 77.67107027571464,\n",
       "  'D': 57.376310697481415,\n",
       "  'L': 56.328755661162},\n",
       " (1, 2): {'U': 96.10112867204197,\n",
       "  'R': 93.47538274406405,\n",
       "  'D': 70.12883609089943,\n",
       "  'L': 61.58467666861638},\n",
       " (1, 3): {'U': 120.7285992406861,\n",
       "  'R': 114.03711327456833,\n",
       "  'D': 79.29018926206925,\n",
       "  'L': 83.26805318048483},\n",
       " (1, 4): {'U': 100,\n",
       "  'R': 117.3475898072442,\n",
       "  'D': 84.14847245455158,\n",
       "  'L': 101.85359036993452},\n",
       " (2, 0): {'U': 59.80400851449698,\n",
       "  'R': 56.810237120522714,\n",
       "  'D': 48.21067190617506,\n",
       "  'L': 51.47567046025362},\n",
       " (2, 1): {'U': 65.40447868279264,\n",
       "  'R': 64.87436796743242,\n",
       "  'D': 49.616040971118565,\n",
       "  'L': 52.04384715878214},\n",
       " (2, 2): {'U': 82.59985364405654,\n",
       "  'R': 78.76505539110963,\n",
       "  'D': 53.94594255915807,\n",
       "  'L': 55.9816814619131},\n",
       " (2, 3): {'U': 101.81722459030262,\n",
       "  'R': 81.70575109443276,\n",
       "  'D': 70.70665073688096,\n",
       "  'L': 63.91268475803936},\n",
       " (2, 4): {'U': 99.77707822801135,\n",
       "  'R': 87.6009364078352,\n",
       "  'D': 63.75131186563663,\n",
       "  'L': 76.92454974716783},\n",
       " (3, 0): {'U': 52.16522514910839,\n",
       "  'R': 49.455868034603895,\n",
       "  'D': 45.06017899194935,\n",
       "  'L': 47.857067768094566},\n",
       " (3, 1): {'U': 56.48403698388689,\n",
       "  'R': 54.08315903761977,\n",
       "  'D': 47.520671650652815,\n",
       "  'L': 47.41667924188762},\n",
       " (3, 2): {'U': 66.68542848944152,\n",
       "  'R': 63.49998745284498,\n",
       "  'D': 49.00457072158969,\n",
       "  'L': 50.79307907652019},\n",
       " (3, 3): {'U': 89.27590983588743,\n",
       "  'R': 67.81382161148515,\n",
       "  'D': 57.40347139756221,\n",
       "  'L': 58.294485463672885},\n",
       " (3, 4): {'U': 83.28849952526633,\n",
       "  'R': 62.55048351767288,\n",
       "  'D': 56.476117705871324,\n",
       "  'L': 63.11243121500117},\n",
       " (4, 0): {'U': 47.865911699328166,\n",
       "  'R': 46.682957862265816,\n",
       "  'D': 44.756275084358606,\n",
       "  'L': 45.39011479637783},\n",
       " (4, 1): {'U': 50.94009354788038,\n",
       "  'R': 50.33821532981513,\n",
       "  'D': 46.63052729692657,\n",
       "  'L': 45.00269873115017},\n",
       " (4, 2): {'U': 56.673782037073536,\n",
       "  'R': 56.42090947826968,\n",
       "  'D': 50.829658067057906,\n",
       "  'L': 46.900519360378034},\n",
       " (4, 3): {'U': 66.81799483579893,\n",
       "  'R': 56.75464363675775,\n",
       "  'D': 59.39393008985479,\n",
       "  'L': 50.116458698902704},\n",
       " (4, 4): {'U': 66.09485882494012,\n",
       "  'R': 57.33686165472205,\n",
       "  'D': 56.009966836611184,\n",
       "  'L': 58.326369702698216}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_model.Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
